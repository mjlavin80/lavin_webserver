{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blog corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19320"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "blogs = glob.glob(\"../../../blogs/*.xml\")\n",
    "len(blogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract all authors with >= 300 sentences (3347 males, 4282 females)\n",
    "2. Randomly pair authors to produce 1282 male/male pairs, 1283 male/female pairs, and 1499 female/female pairs\n",
    "3. Analyze each pair using “triangle test”\n",
    "4. First 100 sentences of each is training data, last 100 sentences each is test data (tests 1 and 2\n",
    "\n",
    "(random chance performance is 50% by design) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def sent_to_freq(sentences):\n",
    "    #word tokenize\n",
    "    token_lists = [ word_tokenize(i) for i in sentences]\n",
    "    token_list = [item for sublist in token_lists for item in sublist]\n",
    "    #convert to frequencies\n",
    "    freqs = Counter(token_list)\n",
    "    return freqs\n",
    "    \n",
    "def process_blog(sentence_list):\n",
    "    #split into training and test\n",
    "    train = sent_to_freq(sentence_list[0:100])\n",
    "    test = sent_to_freq(sentence_list[-101:-1])\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n"
     ]
    }
   ],
   "source": [
    "metadata = []\n",
    "sentence_dict = {}\n",
    "\n",
    "#loop all\n",
    "counter = 1\n",
    "for blog in blogs:\n",
    "    if counter % 5000 == 0:\n",
    "        print(counter)\n",
    "    blogname = blog.replace(\"/blogs/\", \"\")\n",
    "    meta = blogname.split(\".\")\n",
    "    \n",
    "    with open(blog, encoding=\"latin-1\") as t: \n",
    "        xml = t.read()\n",
    "        text = BeautifulSoup(xml, \"lxml\").text\n",
    "    #sentence tokenize\n",
    "    sent_toke_list = sent_tokenize(text)\n",
    "    row = meta[6:10]\n",
    "    row.append(len(sent_toke_list))\n",
    "    sentence_dict[row[0]] = sent_toke_list\n",
    "    metadata.append(row)\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickled_data/blog_sentences.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentence_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('pickled_data/blog_metadata.pickle', 'wb') as handle2:\n",
    "    pickle.dump(metadata, handle2, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pickled_data/blog_sentences.pickle', 'rb') as handle:\n",
    "    sentence_dict = pickle.load(handle)\n",
    "with open('pickled_data/blog_metadata.pickle', 'rb') as handle2:\n",
    "    metadata = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3569"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(metadata, columns=[\"id\",\"gender\", \"age\", \"category\", \"sentence_count\"])\n",
    "#screen rows with too few sentences\n",
    "long_enough = df.loc[df['sentence_count'] >= 200].reset_index(drop=True)\n",
    "#long_enough.iloc[:10]\n",
    "len(long_enough.loc[long_enough['gender'] == 'male'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "#generate random pairings\n",
    "def random_pairs(long_enough):\n",
    "    male_rows = long_enough.loc[long_enough['gender'] == 'male'].reset_index(drop=True)\n",
    "    female_rows = long_enough.loc[long_enough['gender'] == 'female'].reset_index(drop=True)\n",
    "    male_ids = list(male_rows['id'])\n",
    "    female_ids = list(female_rows['id'])\n",
    "    shuffle(male_ids)\n",
    "    shuffle(female_ids)\n",
    "\n",
    "    #need to do one third male-male, one third, female-female, one third male-female\n",
    "    #get lengths of both\n",
    "    m = len(male_ids)\n",
    "    f = len(female_ids)\n",
    "    #whichever is smaller, get half for m-f, half for alike gender\n",
    "    if m < f:\n",
    "        mf_pair_m = male_ids[0:int(m/3)]\n",
    "        mf_pair_f = female_ids[0:int(m/3)]\n",
    "        #for the larger, pair a random chunk half the length of the smaller and use the rest for f-f\n",
    "        mm = male_ids[int(m/3):]\n",
    "        ff = female_ids[int(m/3):]\n",
    "        #make tuple pairs\n",
    "        mma = [j for i,j in enumerate(mm) if i % 2 == 0]\n",
    "        mmb = [j for i,j in enumerate(mm) if i % 2 != 0]\n",
    "        mm_pairings = list(zip(mma, mmb))\n",
    "\n",
    "        ffa = [j for i,j in enumerate(ff) if i % 2 == 0]\n",
    "        ffb = [j for i,j in enumerate(ff) if i % 2 != 0]\n",
    "        ff_pairings = list(zip(ffa, ffb))\n",
    "    else:    \n",
    "        mf_pair_m = male_ids[0:int(f/3)]\n",
    "        mf_pair_f = female_ids[0:int(f/3)]\n",
    "        #for the larger, pair a random chunk half the length of the smaller and use the rest for f-f\n",
    "        mm = male_ids[int(f/3):]\n",
    "        ff = female_ids[int(f/3):]\n",
    "        #make tuple pairs\n",
    "        mma = [j for i,j in enumerate(mm) if i % 2 == 0]\n",
    "        mmb = [j for i,j in enumerate(mm) if i % 2 != 0]\n",
    "        mm_pairings = list(zip(mma, mmb))\n",
    "\n",
    "        ffa = [j for i,j in enumerate(ff) if i % 2 == 0]\n",
    "        ffb = [j for i,j in enumerate(ff) if i % 2 != 0]\n",
    "        ff_pairings = list(zip(ffa, ffb))\n",
    "    mf_pairings = list(zip(mf_pair_m, mf_pair_f))\n",
    "    return ff_pairings, mm_pairings, mf_pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from application.selective_features import dictionaries_without_features, dictionaries_of_features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "with open('pickled_data/fullstops.pickle', 'rb') as handle2:\n",
    "    fullstops = pickle.load(handle2)\n",
    "\n",
    "def predict_pairs(pairings):    \n",
    "    results = []\n",
    "    #loop pairings\n",
    "    count = 1\n",
    "    for p,s in pairings:\n",
    "        if count % 250 == 0:\n",
    "            print(count)\n",
    "        counters_all = [process_blog(sentence_dict[p]), process_blog(sentence_dict[s])]\n",
    "        #[0] positions are train, [1] are tests\n",
    "        stop_features_train_test_a = dictionaries_of_features(counters_all[0], fullstops)\n",
    "        stop_features_train_test_b = dictionaries_of_features(counters_all[1], fullstops)\n",
    "        all_samples = [stop_features_train_test_a[0], stop_features_train_test_b[0],\n",
    "                       stop_features_train_test_a[1], stop_features_train_test_b[1]]\n",
    "        \n",
    "        #instantiate vectorizer\n",
    "        v = DictVectorizer()\n",
    "        #transform all\n",
    "        X = v.fit_transform(all_samples)\n",
    "        #convert to nonsparse\n",
    "        scaled_vsm = X.toarray()\n",
    "        #print(len(scaled_vsm[2:4]))\n",
    "        #train logistic on first 100 sentences\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(scaled_vsm[0:2], [0,1])\n",
    "        #[0, 1] is always the correct prediction\n",
    "        preds = lr.predict(scaled_vsm[2:4])\n",
    "        probs = lr.predict_proba(scaled_vsm[2:4])\n",
    "        \n",
    "        a = [p, s, \"a\", 0]+[preds[0]] + [probs[0][0], probs[0][1]]\n",
    "        b = [p, s, \"b\", 1]+[preds[1]] + [probs[1][0], probs[1][1]]\n",
    "        #record results\n",
    "        results.append(a)\n",
    "        results.append(b)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ff_pairings, mm_pairings, mf_pairings = random_pairs(long_enough)\n",
    "#mf_results = predict_pairs(mf_pairings)\n",
    "#mm_results = predict_pairs(mm_pairings)\n",
    "#ff_results = predict_pairs(ff_pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "ff_results_all = []\n",
    "mf_results_all = []\n",
    "mm_results_all = []\n",
    "\n",
    "for test in range(21):\n",
    "    print(test+1)\n",
    "    ff_pairings, mm_pairings, mf_pairings = random_pairs(long_enough)\n",
    "    mf_results = predict_pairs(mf_pairings)\n",
    "    mf_results_all.extend(mf_results)\n",
    "    mm_results = predict_pairs(mm_pairings)\n",
    "    mm_results_all.extend(mm_results)\n",
    "    ff_results = predict_pairs(ff_pairings)\n",
    "    ff_results_all.extend(ff_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [\"id_a\",\"id_b\",\"test\", \"true\", \"prediction\", \"prob_0\", \"prob_1\" ]\n",
    "df_results_mf = pd.DataFrame.from_records(mf_results_all, columns=cols)\n",
    "df_results_ff = pd.DataFrame.from_records(ff_results_all, columns=cols)\n",
    "df_results_mm = pd.DataFrame.from_records(mm_results_all, columns=cols)\n",
    "\n",
    "df_results_mf.to_csv('mf.csv')\n",
    "df_results_mm.to_csv('mm.csv')\n",
    "df_results_ff.to_csv('ff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mf_right = len(df_results_mf.loc[df_results_mf['true'] == df_results_mf['prediction']].index)\n",
    "df_mf_wrong = len(df_results_mf.index) - df_mf_right\n",
    "df_ff_right = len(df_results_ff.loc[df_results_ff['true'] == df_results_ff['prediction']].index)\n",
    "df_ff_wrong = len(df_results_ff.index) - df_ff_right\n",
    "df_mm_right = len(df_results_mm.loc[df_results_mm['true'] == df_results_mm['prediction']].index)\n",
    "df_mm_wrong = len(df_results_mm.index) - df_mm_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8535984620929953"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.0*df_mf_right)/len(df_results_mf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464585834333733"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.0*df_mm_right)/len(df_results_mm.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435117249316645"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.0*df_ff_right)/len(df_results_ff.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49980\n",
      "55608\n",
      "49938\n"
     ]
    }
   ],
   "source": [
    "print(df_mm_right+df_mm_wrong)\n",
    "print(df_ff_right+df_ff_wrong)\n",
    "print(df_mf_right+df_mf_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8702, 46906)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ff_wrong, df_ff_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18698087695487198"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "oddsratio, pvalue = stats.fisher_exact([[7674, 42306],[8702, 46906]])\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[7311, 42627]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
