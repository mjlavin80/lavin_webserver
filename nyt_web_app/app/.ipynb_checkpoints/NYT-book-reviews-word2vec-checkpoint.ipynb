{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "application/__init__.py:2: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n",
      "  from flask.ext.sqlalchemy import SQLAlchemy\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests \n",
    "import enchant\n",
    "from application import *\n",
    "from application.models import Metadata, Work, Author\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_rows = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 159)\n"
     ]
    }
   ],
   "source": [
    "female_rows = all_rows.loc[all_rows['assumed_gender'] == 'f']\n",
    "male_rows = all_rows.loc[all_rows['assumed_gender'] == 'm']\n",
    "\n",
    "print(len(male_rows), len(female_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words')\n",
    "stoplist1 = words.text.split(\"\\r\\n\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist2 = set(stopwords.words('english'))\n",
    "\n",
    "stoplist1.extend(stoplist2)\n",
    "\n",
    "fullstops = list(set(stoplist1))\n",
    "\n",
    "def remove_stops(stoplist, wordlist):\n",
    "    result = []\n",
    "    for i in wordlist:\n",
    "        if i not in stoplist:\n",
    "                result.append(i)\n",
    "    return result\n",
    "\n",
    "def spellcheck(wordlist):\n",
    "    result = []\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    for i in wordlist:\n",
    "        if d.check(i) or d.check(i.capitalize()):\n",
    "            result.append(i)\n",
    "         \n",
    "    return result\n",
    "\n",
    "def get_term_tree(list_of_texts, term, nyt_ids_list):\n",
    "    term_list =[]\n",
    "    #normalize ocr errors\n",
    "    for h,i in enumerate(list_of_texts):\n",
    "        #lowercase all\n",
    "        ocr_lower = i.lower()\n",
    "        #tokenize, remove punctuation and numbers, remove tabs, newlines, etc.\n",
    "        ocr_cleaner = ocr_lower.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        doc = nlp(ocr_cleaner)\n",
    "        ocr_tokens = []\n",
    "        #add to term list any lemma or token that matches term, but not both if they are the same\n",
    "        for token in doc:\n",
    "            ocr_tokens.append(unicode(token))\n",
    "        for z, token in enumerate(doc):\n",
    "            if token.lemma_ == term or unicode(token) == term:\n",
    "                context_list = []\n",
    "                if z >= 6:\n",
    "                    context_list.extend(ocr_tokens[z-6:z-1])\n",
    "                else: \n",
    "                    context_list.extend(ocr_tokens[0:z-1])\n",
    "                \n",
    "                try:\n",
    "                    context_list.extend(ocr_tokens[z:z+7])\n",
    "                except:\n",
    "                    context_list.extend(ocr_tokens[z:[len(ocr_tokens)]])\n",
    "                result = {\"lemma\": token.lemma_, \"token\": unicode(token), \"pos\": token.pos_, \"nyt_id\": nyt_ids_list[h] }\n",
    "                term_list.append(result)\n",
    "    return term_list\n",
    "            \n",
    "def clean_text(list_of_texts):\n",
    "    fully_cleaned =[]\n",
    "    #normalize ocr errors\n",
    "    for i in list_of_texts:\n",
    "        #lowercase all\n",
    "        ocr_lower = i.lower()\n",
    "        #tokenize, remove punctuation and numbers, remove tabs, newlines, etc.\n",
    "        ocr_cleaner = ocr_lower.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        doc = nlp(ocr_cleaner)\n",
    "        ocr_tokens = []\n",
    "        for token in doc:\n",
    "            \n",
    "            if token.lemma_ == u'-PRON-' or token.lemma_.isupper():\n",
    "                ocr_tokens.append(unicode(token))\n",
    "            else:\n",
    "                ocr_tokens.append(token.lemma_)\n",
    "        #ocr_tokens = ocr_cleaner.split(\" \")\n",
    "        \n",
    "        no_numbers_or_punct = []\n",
    "        for token in ocr_tokens:\n",
    "            if token.isalpha():\n",
    "                no_numbers_or_punct.append(token)\n",
    "            else:\n",
    "                \n",
    "                new_token = \"\"\n",
    "                for letter in token:\n",
    "                    if letter.isalpha():\n",
    "                        new_token += letter\n",
    "                if new_token != \"\":\n",
    "                    no_numbers_or_punct.append(new_token)  \n",
    "        \n",
    "        \n",
    "        spellchecked = spellcheck(no_numbers_or_punct)\n",
    "        fully_cleaned.append(spellchecked)\n",
    "    return fully_cleaned\n",
    "\n",
    "def clean_text_sentences(list_of_texts):\n",
    "    fully_cleaned =[]\n",
    "    #normalize ocr errors\n",
    "    for i in list_of_texts:\n",
    "        #lowercase all\n",
    "        ocr_lower = i.lower()\n",
    "        #tokenize, remove punctuation and numbers, remove tabs, newlines, etc.\n",
    "        ocr_cleaner = ocr_lower.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        doc = nlp(ocr_cleaner)\n",
    "        \n",
    "        sentences = [sent for sent in doc.sents]\n",
    "        \n",
    "        sentences_tokenized = []\n",
    "        for s in sentences:\n",
    "            sentence_tokens = []\n",
    "            for token in s:\n",
    "                \n",
    "                if token.lemma_ == u'-PRON-' or token.lemma_.isupper():\n",
    "                    sentence_tokens.append(unicode(token))\n",
    "                else:\n",
    "                    sentence_tokens.append(token.lemma_)    \n",
    "            sentences_tokenized.append(sentence_tokens)\n",
    "        \n",
    "        no_numbers_or_punct = []\n",
    "        for sentence in sentences_tokenized:\n",
    "            no_numbers_or_punct_sentence = []\n",
    "            for token in sentence:\n",
    "                if token.isalpha():\n",
    "                    no_numbers_or_punct_sentence.append(token)\n",
    "                else:\n",
    "\n",
    "                    new_token = \"\"\n",
    "                    for letter in token:\n",
    "                        if letter.isalpha():\n",
    "                            new_token += letter\n",
    "                    if new_token != \"\":\n",
    "                        no_numbers_or_punct_sentence.append(new_token)\n",
    "            \n",
    "            no_numbers_or_punct.append(no_numbers_or_punct_sentence)\n",
    "        \n",
    "        spellchecked = [spellcheck(i) for i in no_numbers_or_punct]\n",
    "        fully_cleaned.extend(spellchecked)\n",
    "    return fully_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fullstops[0]\n",
    "\n",
    "#this list of gender terms was generated iteratively by running the logistic regression with all terms, \n",
    "#seeing what correlated the most with gender, and removing words that seemed to have direct gender info in them\n",
    "\n",
    "gender_terms = [\"mr\", \"he\", \"his\", \"him\", \"himself\", \"man\", \"men\", \"boy\", \"boys\", \"manly\", \"masculine\", \"boyish\", \"father\", \\\n",
    "                \"brother\", \"girls\", \"men\", \"women\", \"sisters\", \"daughters\", \"brothers\", \"sons\", \"wife\", \"husband\", \"niece\",\\\n",
    "                \"uncle\", \"nephew\", \"dad\", \"grandfather\", \"son\", \"mrs\", \"miss\", \"her\", \"hers\", \"she\", \"herself\", \"woman\",\\\n",
    "                \"girl\", \"nieces\", \"nephews\", \"fer\", \"mme\", \"mlle\", \\\n",
    "                \"lady\", \"womanly\", \"girlish\", \"girly\", \"mother\", \"daughter\", \"aunt\", \"niece\" \"grandmother\", \"mom\", \"sister\" ]\n",
    "\n",
    "from nltk.corpus import names \n",
    "male = [o.lower() for o in names.words('male.txt')]\n",
    "female = [o.lower() for o in names.words('female.txt')]\n",
    "\n",
    "fullstops_and_pronouns = []\n",
    "\n",
    "for u in [fullstops, gender_terms]:\n",
    "    for i in u:\n",
    "        fullstops_and_pronouns.append(unicode(i))\n",
    "\n",
    "fullstops_and_pronouns = list(set(fullstops_and_pronouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 396, 7982)\n"
     ]
    }
   ],
   "source": [
    "from string import ascii_lowercase\n",
    "fullstops_pronouns_and_names = []\n",
    "\n",
    "for u in [fullstops_and_pronouns, male, female]:\n",
    "    for i in u:\n",
    "        fullstops_pronouns_and_names.append(unicode(i))\n",
    "\n",
    "fullstops_pronouns_and_names.append(unicode(\"thoma\"))\n",
    "\n",
    "for ltr in ascii_lowercase:\n",
    "    fullstops_pronouns_and_names.append(unicode(ltr))\n",
    "\n",
    "fullstops_pronouns_and_names = list(set(fullstops_pronouns_and_names))\n",
    "\n",
    "print(len(fullstops), len(fullstops_and_pronouns), len(fullstops_pronouns_and_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_list_all = []\n",
    "nyt_ids = []\n",
    "for i in all_rows.iterrows():\n",
    "    row = Metadata().query.filter(Metadata.id == int(i[1][0])).one_or_none()\n",
    "    ocr_list_all.append(row.ocr_transcription)\n",
    "    nyt_ids.append(row.nyt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_cleaned_sentences = clean_text_sentences(ocr_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_cleaned_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
