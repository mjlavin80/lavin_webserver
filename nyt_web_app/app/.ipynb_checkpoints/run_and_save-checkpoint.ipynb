{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "ocr_counters = pickle.load( open( \"ocr_counters_all.pickle\", \"rb\" ) )\n",
    "fullstops = pickle.load( open( \"fullstops.pickle\", \"rb\" ) )\n",
    "nyt_ids_all = pickle.load( open( \"nyt_ids_all.pickle\", \"rb\" ) )\n",
    "from application.selective_features import dictionaries_without_features\n",
    "ocr_counters_no_stops_or_names = dictionaries_without_features(ocr_counters, fullstops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    \n",
    "def predict_genders(nyt_ids, ocr_counters, r):\n",
    "\n",
    "    v = DictVectorizer()\n",
    "    X = v.fit_transform(ocr_counters)\n",
    "    y = TfidfTransformer()\n",
    "    Z = y.fit_transform(X)\n",
    "\n",
    "    scaled_vsm = Z.toarray()\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "    except:\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    y_as_list = list(range(len(scaled_vsm)))\n",
    "    train_size_int = 435\n",
    "    test_size_int = 100\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_vsm, y_as_list, train_size=train_size_int, \\\n",
    "                                                        test_size=test_size_int, random_state=r)\n",
    "    train_labels = []\n",
    "    # 0 = male here ... eventually I'll need to make this dynamic or external,for now it's set number of each\n",
    "    for pos in y_train:\n",
    "        if pos < 414:\n",
    "            value = 0\n",
    "        else:\n",
    "            value = 1\n",
    "        train_labels.append(value)\n",
    "    test_labels = []\n",
    "    for pos in y_test:\n",
    "        if pos < 414:\n",
    "            value = 0\n",
    "        else:\n",
    "            value = 1\n",
    "        test_labels.append(value)\n",
    "    lr = LogisticRegression(class_weight={0:1.21, 1:4.14})\n",
    "    lr.fit(X_train, train_labels)\n",
    "    results = lr.predict(X_test)\n",
    "    probs = lr.predict_proba(X_test)\n",
    "    result_rows = zip(nyt_ids, results, test_labels, [i[0] for i in probs], [i[1] for i in probs])\n",
    "    \n",
    "    score = lr.score(X_test, test_labels)\n",
    "    y_train = [str(u) for u in y_train]\n",
    "    y_test = [str(z) for z in y_test]\n",
    "    a = ', '.join(y_train)\n",
    "    b = \", \".join(y_test)\n",
    "    #build main_row\n",
    "    main_row = [a,b, train_size_int, test_size_int, score]\n",
    "    #train_ids, test_ids, train_size, test_size, accuracy, stopwords, features, weighting\n",
    "    \n",
    "    words = v.vocabulary_.keys()\n",
    "    labeled_coefs = {}\n",
    "    for i in words:\n",
    "        try:\n",
    "            labeled_coefs[i] = lr.coef_[0][v.vocabulary_[i]]\n",
    "        except: \n",
    "            pass\n",
    "    coef_tuples = labeled_coefs.items()\n",
    "    import numpy as np\n",
    "    odds_f = np.exp([i[1] for i in coef_tuples])\n",
    "    odds_m = np.exp([i[1]*-1.0 for i in coef_tuples])\n",
    "    terms = [i[0] for i in coef_tuples]\n",
    "    coefs = [i[1] for i in coef_tuples]\n",
    "    coef_rows = zip(terms, coefs, odds_m, odds_f)\n",
    "    return main_row, result_rows, coef_rows\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def store_results(main_row, result_rows, coef_rows):\n",
    "    conn = sqlite3.connect('regression_scores.db')\n",
    "    c = conn.cursor()\n",
    "    make_main = \"\"\"CREATE TABLE IF NOT EXISTS main \n",
    "    (id INTEGER PRIMARY KEY, train_ids TEXT, test_ids TEXT, train_size INTEGER, \n",
    "    test_size INTEGER, accuracy REAL, stopwords TEXT, features TEXT, weighting TEXT)\"\"\"\n",
    "    c.execute(make_main)\n",
    "    make_results = \"\"\"CREATE TABLE IF NOT EXISTS results (id INTEGER PRIMARY KEY, \n",
    "    main_id INTEGER, nyt_id TEXT, predicted INTEGER, labeled INTEGER, probability_m REAL, probability_f REAL, \n",
    "    FOREIGN KEY(main_id) REFERENCES main(id))\"\"\"\n",
    "    c.execute(make_results)\n",
    "    make_coefficients = \"\"\"CREATE TABLE IF NOT EXISTS coefficients (id INTEGER PRIMARY KEY, \n",
    "    main_id INTEGER, term TEXT, coefficient_score REAL, odds_m REAL, odds_f REAL, \n",
    "    FOREIGN KEY(main_id) REFERENCES main(id))\"\"\"\n",
    "    c.execute(make_coefficients)\n",
    "    \n",
    "    insert_main = \"\"\"INSERT INTO main (id, test_ids, train_ids, train_size, test_size, \n",
    "    accuracy, stopwords, features, weighting) \n",
    "    VALUES (null, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\"\n",
    "    c.execute(insert_main, main_row)\n",
    "    conn.commit()\n",
    "\n",
    "    #get id for row you just inserted\n",
    "    main_id = c.execute(\"\"\"SELECT id FROM main ORDER BY id DESC\"\"\").fetchone()[0]\n",
    "    insert_result = \"\"\"INSERT INTO results (id, main_id, nyt_id, predicted, labeled, probability_m, \n",
    "    probability_f) \n",
    "    VALUES (null, ?, ?, ?, ?, ?, ?)\"\"\"\n",
    "    for result_row in result_rows:\n",
    "        new_row = [main_id]\n",
    "        new_row.extend(result_row)\n",
    "        c.execute(insert_result, new_row)\n",
    "    conn.commit()\n",
    "    insert_coef = \"\"\"INSERT INTO coefficients (id, main_id, term, coefficient_score, odds_m, odds_f) \n",
    "    VALUES (null, ?, ?, ?, ?, ?)\"\"\"\n",
    "    \n",
    "    for coef_row in coef_rows:\n",
    "        new_c_row = [main_id]\n",
    "        new_c_row.extend(coef_row)\n",
    "        c.execute(insert_coef, new_c_row)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "for z in range(500):\n",
    "    if z % 10 == 0:\n",
    "        print(z)\n",
    "    main_row, result_rows, coef_rows = predict_genders(nyt_ids_all, ocr_counters, z)\n",
    "    main_row.extend([\"none\", \"term frequencies\", \"tf-idf\"])\n",
    "    store_results(main_row, result_rows, coef_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((u'4fc045fc45c1498b0d2162b6', 0, 0, 0.77889788359923384, 0.22110211640076619), (u'stock', -0.040830173400804332, 1.0416751863687461, 0.95999214830678181))\n"
     ]
    }
   ],
   "source": [
    "print(main_row, result_rows[0], coef_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "for z in range(500):\n",
    "    if z % 10 == 0:\n",
    "        print(z)\n",
    "    main_row, result_rows, coef_rows = predict_genders(nyt_ids_all, ocr_counters_no_stops_or_names, z)\n",
    "    main_row.extend([\"stops, gender terms, names\", \"term frequencies\", \"tf-idf\"])\n",
    "    store_results(main_row, result_rows, coef_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
